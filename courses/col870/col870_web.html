<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Instructor: Adarsh Barik Credit: 3 (3-0-0) Semester 2 (2025-2026) TF 3:30-5 PM" />
  <title>COL870/8385: Special Topics in Machine Learning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="simple1.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">COL870/8385: Special Topics in Machine Learning</h1>
<p class="author">Instructor: <a href="https://adarsh-barik.github.io/">Adarsh Barik</a><br />
Credit: 3 (3-0-0)<br />
Semester 2 (2025-2026)<br />
LH 620<br />
TF 3:30-5 PM</p>
</header>
<p style="text-align: center; color: #58C6C0; font-size: 160%">Optimization for Machine Learning</p>
<p>COL870/COL8385 is a 3-credit Special Topics course in Machine Learning. The course will cover topics in optimization in both offline and online settings. The material will be motivated throughout by applications to modern machine learning problems, and will include both foundational ideas and advanced topics.</p>
<h1 class="unnumbered" id="expected-background">Expected background</h1>
<p>This course is intended for both <strong>(post)graduate and undergraduate students</strong> interested in the optimization foundations of machine learning. A basic level of mathematical maturity is expected; students with concerns about their background should consult the instructor.</p>
<p>A fundamental understanding of linear algebra, calculus, and probability theory is required for this course. Prior experience or familiarity with machine learning is also beneficial, though not mandatory.</p>
<h1 class="unnumbered" id="learning-outcomes">Learning outcomes</h1>
<p>Upon completing this course, students will be familiar with key concepts in optimization, enabling them to:</p>
<ol>
<li><p>Understand the fundamental concepts such as convex sets, convex functions and optimality criteria for the optimization problems</p></li>
<li><p>Understand first-order optimization algorithms and analyze their convergence properties</p></li>
<li><p>Gain familiarity with the foundational concepts in online learning</p></li>
</ol>
<h1 class="unnumbered" id="content">Content</h1>
<p>Key topics include convex sets and functions, conjugates, subdifferentials, primal and dual problem formulations, strong and weak duality, minimax characterizations, and optimality conditions including the Karush-Kuhn-Tucker (KKT) criteria. The course will also cover first-order optimization methods such as gradient descent, stochastic gradient descent (SGD), accelerated gradient techniques, subgradient methods, and Frank-Wolfe algorithms. Additionally, foundational concepts in online learning will be explored, including online gradient descent, online mirror descent, Follow-The-Regularized-Leader (FTRL), and parameter-free algorithms.</p>
<h1 class="unnumbered" id="tentative-grading-scheme">Tentative Grading Scheme</h1>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">Midsemester Exam</td>
<td style="text-align: left;">30%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Final Exam</td>
<td style="text-align: left;">30%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Assignments</td>
<td style="text-align: left;">25%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scribe</td>
<td style="text-align: left;">10%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">In-class participation</td>
<td style="text-align: left;">5%</td>
</tr>
</tbody>
</table>
<h2 class="unnumbered" id="audit-policy">Audit Policy</h2>
<p>Minimum 75% attendance and marks equivalent to a grade B or above will be considered as audit pass.</p>
<h2 class="unnumbered" id="scribe">Scribe</h2>
<ul>
<li><p>Each student will be assigned a lecture to scribe.</p></li>
<li><p>Scribing must be done in LaTeX, and I will provide the required <a href="template/scribe_template.tex">scribe template</a>.</p></li>
<li><p>Scribe will be due within one week of the lecture date.</p></li>
</ul>
<h2 class="unnumbered" id="in-class-participation">In-class participation</h2>
<ul>
<li><p>The grade for in-class participation will be based on both attendance and active engagement in class discussions.</p></li>
<li><p>The first five minutes of each lecture will be devoted to reviewing the summary and unsolved exercises from the previous lecture, providing an opportunity for students to contribute to the discussion.</p></li>
</ul>
<h1 class="unnumbered" id="collaboration-and-use-of-ai-tools">Collaboration and use of AI Tools</h1>
<ul>
<li><p>Collaboration and discussion among students are strongly encouraged. However, all deliverables will be graded individually. You may acknowledge your collaborators by including their names in the acknowledgement section.</p></li>
<li><p>Plagiarism in any form (including the use of AI tools) is strictly prohibited. Any violation of this policy will result in a score of <span class="math inline">0</span> for the entire course.</p></li>
</ul>

<h1 class="unnumbered" id="lecture-notes-and-assignments">Lecture Notes and Assignments</h1>
<ol>                
	<li><p>Jan 2, 2026: Introduction, Convex Sets and Convex Functions (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQAq_ztC85GvQode9P65GHFkAZz5vzdyPCuocuR318Gox_0?e=jkLFN4">handwritten notes</a>, <a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQCB_z5juUM5TpUkyHTiKOhxAS9OqZRoyh6PdvWLcxMwvpI?e=1TcN4W">scribe</a>, [BV] Chap 2-3)</p></li>
	<li><p>Jan 6, 2026: Convex Functions and conjugates (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQCP05e7TN1QTp5kdcMeY71FAS6GIUhf7p0gfEZAQ42BJOY?e=nucdSD">handwritten notes</a>, <a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQBjHbQM6svNQ6Sc8hNXNvQ5AQzLTCVistXtx7UV5Diqfx4?e=5MvrQw">scribe</a>, [BV] Chap 3)</p></li>
	<li><p>Jan 9, 2026: Conjugates, Beyond Convexity (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQCkOq2o_Z2CQ44CS3KQpov0Afc3WoVA-u-fi2OglUyEiNs?e=Se3g3h">handwritten notes</a>, [BV] Chap 3) and Convex Optimization Problems (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQD1uI2N9D-tTaXPm7zfdEyZAf7vTvcjQlUbrjes0dVH-AA?e=dxXFYR">handwritten notes</a>, <a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQApz3oX3wqzRYocmDoYXrhOAWa9l51Nm-vjYBAMl4Nc1xI?e=vRil89">scribe</a>, [BV] Chap 4)</p></li>
	<li><p>Jan 13, 2026: Duality (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQA2vvQfyNYkRZfNbZRkZ1WbARmGSa3ZRxEeh1BkszOL8Yw?e=KvZaLI">handwritten notes</a>, <a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQAjstTdp-8jRb7WUuhGZUimARRB7PNvNLt6lBAmVgtpyzU?e=PfFyth">scribe</a>, [BV] Chap 5)</p></li>
	<li><p>Jan 16, 2026: Duality contd. (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQDdFxrE4BdpSYuxFmcUOAAZAdgLPoS32vWsRB607oRjFWk?e=sGLI9V">handwritten notes</a>), KKT Conditions (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQDgu_nzqu4oQq5r0PTMcFB5ASQnp2hG2oy_InjQUJdcBJk?e=A0tJA7">handwritten notes</a>, scribe)</p></li>
	<li><p>Jan 20, 2026: KKT Conditions contd. (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQCXZz0fGKy2RY-y9SDzWZDeAVIChhApgZoKmWs_AyFge9s?e=Fl7fsi">handwritten notes</a>), First-Order Methods (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQBIUPvXpLnNQILRjSrt7N50AWtLN1TURn9hXLoNCOWeqaE?e=uJrutp">handwritten notes</a>, scribe)</p></li>
	<li><p>Jan 23, 2026: Convergence of First-Order Methods (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQBSzfjssfQdSYA9HIvduyFXAcCvH9lOWNnWre7pkOq1mNY?e=f1VAZq">handwritten notes</a>, scribe, [BG])</p></li>
	<li><p>Jan 27, 2026: Convergence (Lipschitz-continuous functions) (<a href="https://csciitd-my.sharepoint.com/:b:/g/personal/adarshbarik1_iitd_ac_in/IQBH_oOPuUK1R4jlCVcmGyYqAbsSWaKttuDcNC-uO6ksYuQ?e=MgI1nA">handwritten notes</a>, scribe, [BG])</p></li>
</ol>    

<h1 class="unnumbered" id="reference-textbook">Reference textbook</h1>
<ol>
<li><p> [BV] Convex Optimization. S. Boyd and L. Vandenberghe. Cambridge University Press, Cambridge, 2003</p></li>
<li><p>[N] Introductory Lectures on Convex Optimization: A Basic Course. Y. Nesterov. Kluwer, 2004.</p></li>
<li><p>[NW] Numerical Optimization. J. Nocedal and S. J. Wright, Springer Series in Operations Research, Springer-Verlag, New York, 2006 (2nd edition).</p></li>
<li><p>[O] A Modern Introduction to Online Learning. Francesco Orabona. arXiv preprint arXiv:1912.13213 (2019).</p></li>
<li><p>[H] Introduction to Online Convex Optimization. Elad Hazan. arXiv preprint arXiv:1909.05207 (2019).</p></li>
<li><p>[BG] Potential-Function Proofs for Gradient Methods. Nikhil Bansal, Anupam Gupta. arXiv preprint arXiv:1712.04581 (2017).</p></li>
</ol>
</body>
</html>
