<!DOCTYPE html>
<!-- saved from url=(0035)https://web.ics.purdue.edu/~abarik/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="tag.css">
        <title>
            Adarsh's webpage
        </title>
        <!--
            Some color codes:
            blueberry : #3892e0
            strawberry : #da4d45
            orange : #f37329
            banana : #fbd25d
            lime : #93d844
            grape : #8a4ebf
            white : #ffffff
            black : #333333

        -->
<style>
    .beautiful-box {
      max-width: 100%;
      margin: 20px auto;
      background: #f9fafc;
      border-radius: 16px;
      box-shadow: 0 4px 24px rgba(60,62,126,0.2);
      padding: 5px;
      text-align: center;
      /* font-family: 'Segoe UI', Arial, sans-serif;*/
      color: #404080;
      transition: box-shadow 0.3s;
    }
    .beautiful-box:hover {
      box-shadow: 0 8px 30px rgba(60,62,126,0.3);
    }
    .beautiful-box h2 {
      margin-top: 0;
      font-size: 120%;
    }
    .beautiful-box p {
      margin-bottom: 0;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
    }
    .headertext {
	color: #333333;
	font-family: 'Poppins', Arial, Helvetica, sans-serif; 
	font-weight: bold; 
	/* text-align: center; */
	font-size: 200%;
	     }
    .subheadertext {
	color: #333333;
	font-family: 'Poppins', Arial, Helvetica, sans-serif; 
	font-weight: normal;
	font-size: 150%;
	     }
    .normaltext {
	color: #333333;
	font-family: Arial, Helvetica, sans-serif;
	font-weight: normal;
	font-size: 100%;
	     }
    .alerttext {
	color: #da4d45;
	font-family: Arial, Helvetica, sans-serif;
	font-weight: normal;
	font-size: 100%;
	     }
  </style>

  <style>
    .beautiful-box-news {
      max-width: 100%;
      margin: 20px auto;
      background: #f9fafc;
      /* background: radial-gradient(circle, #e0f7fa 60%, #e8f5e9 100%);*/
      border-radius: 16px;
      box-shadow: 0 4px 24px rgba(60,62,126,0.15);
      padding: 5px;
      text-align: center;
      /* font-family: 'Segoe UI', Arial, sans-serif;*/
      color: #366c84;
      transition: box-shadow 0.3s;
    }
    .beautiful-box-news h2 {
      margin-top: 0;
      font-size: 120%;
    }
    .item-list {
      list-style: none;
      padding: 0;
      margin: 10px 0 5px 0;
      text-align: left;
    }
    .item-list li {
      background: rgba(180, 220, 220, 0.22);
      margin: 5px 0;
      padding: 2px;
      border-radius: 10px;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
      transition: background 0.2s;
    }
    .see-more-btn {
      cursor: pointer;
      background: #b3e5fc;
      border: none;
      color: #366c84;
      padding: 2px 4px;
      border-radius: 4px;
      margin-top: 2px;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
      transition: background 0.2s;
    }
    .see-more-btn:hover {
      background: #81d4fa;
    }
    .hidden {
      display: none;
    }
  </style>

        <style>
            <!-- @font-face { -->
            <!--         font-family: quicksand; -->
            <!--                 src: url(fonts/Quicksand-Regular.otf); -->
            <!--            } -->
            <!-- @font-face { -->
            <!--         font-family: quicksandregular; -->
            <!--                 src: url(fonts/Quicksand-Regular.otf); -->
            <!--            } -->
            <!-- .lcolumn { -->
            <!--     position: fixed; -->
            <!-- } -->
			<!-- .rcolumn { -->
			<!--     width: 70%; -->
			<!--     position: relative; -->
			<!-- } -->
			/* Clear floats after the columns */
            .unscrollable {
                position: fixed;
            }
            .scrollable {
                position: relative;
            }
			.row:after {
				content: "";
				display: table;
				clear: both;
			}
            .headertext {
                color: #333333;
                font-family: 'Poppins', Arial, Helvetica, sans-serif;
                font-weight: bold;
                /* text-align: center; */
                font-size: 200%;
                     }
            .subheadertext {
                color: #333333;
                font-family: 'Poppins', Arial, Helvetica, sans-serif;
                font-weight: normal;
                font-size: 150%;
                     }
            .normaltext {
                color: #333333;
                font-family: Arial, Helvetica, sans-serif;
                font-weight: normal;
                font-size: 100%;
                     }
            .alerttext {
                color: #da4d45;
                font-family: Arial, Helvetica, sans-serif;
                font-weight: normal;
                font-size: 100%;
                     }
            body {
                margin-top: 3%;
                margin-right: 15%;
                margin-left: 15%;
                }
            a:link {
                text-decoration: none;
                color: #3892e0;
            }
            a:visited {
                text-decoration: none;
                color: #8a4ebf;
            }
            a:hover {
                text-decoration: none;
                color: #93d844;
            }

            #footer
            {
                position:fixed;
                bottom:0px;
                left:0px;
                right:0px;
                height:15px;
                margin-bottom:0px;
                text-align: center;
                color: #333333;
                background-color: #ffffff;
                font-family: Arial, Helvetica, sans-serif;
                font-size: small;
            }
            @media only screen and (max-width:767px)
            {
                #one, #two
                {
                    display: block;
                    float: none;
                    width: 100%;
                }
            }
        </style>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
            });
        </script>
        <script type="text/javascript"
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>
    <body>
              <div class="wrapper">
              <header>
					<h2 class="headertext">Adarsh Barik</h2>
					<p><img src="./ad_dp1.jpg" style="max-width:50%"></p>
					<h2 class="subheadertext">Contact</h2>
					<p class="normaltext">
                    Assistant Professor <br>
					Department of Computer Science and Engineering <br>
					Indian Institute of Technology Delhi <br>
                    Office: Bharti-422 <br>
                    Email: adarshbarik1(at)cse(dot)iitd(dot)ac(dot)in <br>
                    <!-- CV: <a href="adarsh-resume.pdf"> Click Here  </a> -->
					</p>
                    <hr>
                    <p>
                    <div class="tagbutton">
                        <a class="tag" href="#about"><span>About</span></a>
                    </div>
                    <div class="tagbutton">
                        <a class="tag" href="#research"><span>Research</span></a>
                    </div>
                    <div class="tagbutton">
                        <a class="tag" href="#publications"><span>Publications</span></a>
                    </div>
                    <br>
                    <div class="tagbutton">
                        <a class="tag" href="#teaching"><span>Teaching</span></a>
                    </div>
                    <div class="tagbutton">
                        <a class="tag" href="#academic"><span>Academic Service</span></a>
                    </div>
                    </p>
              </header>
              <section>
		<div class="beautiful-box">
		    <h2>Announcement</h2>
		    <p>
		    I am actively seeking motivated PhD and master's students interested in working in the fields of optimization (both online and offline) and statistical learning theory. If you wish to apply, please send me an email with your CV. </p>

		    <p>
Undergraduate students from IIT Delhi are also welcome to apply for long-term projects. However, I am currently not offering short-term projects or internships.

Due to the high volume of emails and limited availability, I may not be able to respond to every inquiry.
		</p>
		</div>
					<h2 id="about" class="subheadertext">About Me</h2>

                    <p class="normaltext">I am an Assistant Professor in the <a href="https://homecse.iitd.ac.in/">Department of Computer Science and Engineering</a> at <a href="https://home.iitd.ac.in/">Indian Institute of Technology Delhi (IIT Delhi)</a>. Prior to joining IIT Delhi, I was a Research Fellow in the <a href="https://ids.nus.edu.sg/">Institute of Data Science</a> at the <a href="https://www.nus.edu.sg/">National University of Singapore (NUS)</a>, where I was hosted by <a href="https://vyftan.github.io/index.html">Prof. Vincent Y. F. Tan</a>. I obtained my Ph.D. from  <a href="https://www.cs.purdue.edu/">Department of Computer Science</a> at <a href="http://www.purdue.edu/">Purdue University</a> with <a href="https://www.cs.purdue.edu/homes/jhonorio/index.html">Prof. Jean Honorio</a> as my advisor and completed my B.Tech and M.Tech (Dual Degree) from <a href="https://www.iitm.ac.in/">Indian Institute of Technology, Madras</a>.</p>
                    <!-- <p class="alerttext"> I am looking for a post doc position starting from Summer 2023. You can find my <a href="adarsh-resume.pdf"> CV here. </a> </p> -->
<!-- in 2013, with a major in <a href="http://www.ae.iitm.ac.in/index.htm">Aerospace Engineering</a> and a minor in <a href="http://www.doms.iitm.ac.in/">Operations Research</a>. My Dual Degree thesis advisor was <a href="http://www.ae.iitm.ac.in/~krishna/ramakrishnam.html">Prof. M. Ramakrishna</a> .</p> -->




				<h2 id="research" class="subheadertext">Research Interests</h2>
				<p class="normaltext">
				Theoretical and computational aspects of Optimization, Machine Learning, High-Dimensional Statistics, and Information Theory.
				</p>
				<p class="normaltext">
                I am interested in solving optimization problems in both online and offline setting. My main focus revolves around developing provably correct learning algorithms whith theoretical gurantees related to convergence, sample complexity and computational complexity for machine learning problems.                 </p>

                <!-- In past, I have worked on providing theoretical guarantees for combinatorial problems using continuous relaxation.
For example, I have worked on <a href="https://arxiv.org/abs/1905.12552">learning Bayesian networks with low rank conditional probability tables</a>.  My research expands beyond the confines of convex problems. As an example of that, I have worked on developing provable theoretical bounds for <a href="https://arxiv.org/abs/2102.09704">fair sparse regression problem using invex relaxation</a>. -->
				<!-- </p> -->

	<div class="beautiful-box-news">
	    <h2>News</h2>
	    <ul class="item-list">
		    <li><b>2025:</b> Paper "<a href="papers/sea_parafree.pdf">Parameter-free Algorithms for the Stochastically Extended Adversarial Model</a>" accepted to NeurIPS 2025. Joint work with <a href="https://scholar.google.com/citations?user=1qUSRCcAAAAJ&hl=en">Shuche Wang</a>, <a href="https://www.lamda.nju.edu.cn/zhaop/">Peng Zhao</a> and <a href="https://vyftan.github.io/">Vincent Y.F. Tan</a>.</li>
		    <li><b>2025:</b> Paper "<a href="https://arxiv.org/abs/2409.04733">A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase Retrieval</a>" accepted to the IEEE Transactions on Information Theory. Joint work with <a href="https://anandkrishna.me/">Anand Krishna</a> and <a href="https://vyftan.github.io/">Vincent Y.F. Tan</a>.</li>
		    <li><b>2025:</b> I have joined <a href="https://homecse.iitd.ac.in/">Department of Computer Science and Engineering</a> at <a href="https://home.iitd.ac.in/">IIT Delhi</a> as an Assistant Professor. Excited about working with amazing faculty and talented students here.</li>
		    <li class="hidden"><b>2025:</b> Paper "<a href="https://arxiv.org/abs/2208.09449">On Exact Solutions of the Inner Optimization Problem of Adversarial Robustness</a>" accepted to the IEEE ICASSP. Joint work with <a href="https://d-maurya.github.io/">Deepak Maurya</a> and <a href="https://www.cs.purdue.edu/homes/jhonorio/index.html">Jean Honorio</a>.</li>
		    <li class="hidden"><b>2025:</b> Paper "<a href="https://arxiv.org/abs/2412.10751">p-Mean Regret for Stochastic Bandits</a>" accepted to the AAAI. Joint work with <a href="https://anandkrishna.me/">Anand Krishna</a>, Philips George John, and <a href=""https://vyftan.github.io/"">Vincent Y.F. Tan</a>.</li>
	    </ul>
	    <button class="see-more-btn" onclick="showMore()">See more</button>
	  </div>
	  <script>
	    function showMore() {
	      const hiddenItems = document.querySelectorAll('.item-list .hidden');
	      hiddenItems.forEach(li => li.classList.remove('hidden'));
	      document.querySelector('.see-more-btn').style.display = 'none';
	    }
	  </script>


				<h2 id="publications" class="subheadertext">Publications</h2>
				<p class="normaltext">
				</p><ol class="normaltext">


					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2408.06297"> LEARN: An Invex Loss for Outlier Oblivious Robust Online Optimization </a>
</summary>
<p>
<em>
    We study a robust online convex optimization framework, where an adversary can introduce outliers by corrupting loss functions in an arbitrary number of rounds k, unknown to the learner. Our focus is on a novel setting allowing unbounded domains and large gradients for the losses without relying on a Lipschitz assumption. We introduce the Log Exponential Adjusted Robust and iNvex (LEARN) loss, a non-convex (invex) robust loss function to mitigate the effects of outliers and develop a robust variant of the online gradient descent algorithm by leveraging the LEARN loss. We establish tight regret guarantees (up to constants), in a dynamic setting, with respect to the uncorrupted rounds and conduct experiments to validate our theory. Furthermore, we present a unified analysis framework for developing online optimization algorithms for non-convex (invex) losses, utilizing it to provide regret bounds with respect to the LEARN loss, which may be of independent interest.
</em>
</p>
</details>
						<br>
						Adarsh Barik, Anand Krishna, Vincent Y. F. Tan<br>
						Preprint
					</li>

					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2510.04685"> Parameter-free Algorithms for the Stochastically Extended Adversarial Model </a>
</summary>
<p>
<em>
	We develop the first parameter-free algorithms for the Stochastically Extended Adversarial (SEA) model, a framework that bridges adversarial and stochastic online convex optimization. Existing approaches for the SEA model require prior knowledge of problem-specific parameters, such as the diameter of the domain $D$ and the Lipschitz constant of the loss functions $G$, which limits their practical applicability. Addressing this, we develop parameter-free methods by leveraging the Optimistic Online Newton Step (OONS) algorithm to eliminate the need for these parameters. We first establish a comparator-adaptive algorithm for the scenario with unknown domain diameter but known Lipschitz constant, achieving an expected regret bound of $\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} + \sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic variance and cumulative adversarial variation, respectively. We then extend this to the more general setting where both $D$ and $G$ are unknown, attaining the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$, demonstrating the efficacy of our proposed methods even when both parameters are unknown in the SEA model.
</em>
</p>
</details>
						<br>
						Shuche Wang, Adarsh Barik, Peng Zhao, Vincent Y. F. Tan<br>
						Accepted, NeurIPS 2025
					</li>

					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2409.04733"> A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase Retrieval </a>
</summary>
<p>
<em>
    In this work, we study the robust phase retrieval problem where the task is to recover an unknown signal $\theta^* \in \mathbb{R}^d$ in the presence of potentially arbitrarily corrupted magnitude-only linear measurements. We propose an alternating minimization approach that incorporates an oracle solver for a non-convex optimization problem as a subroutine. Our algorithm guarantees convergence to $\theta^*$ and provides an explicit polynomial dependence of the convergence rate on the fraction of corrupted measurements. We then provide an efficient construction of the aforementioned oracle under a sparse arbitrary outliers model and offer valuable insights into the geometric properties of the loss landscape in phase retrieval with corrupted measurements. Our proposed oracle avoids the need for computationally intensive spectral initialization, using a simple gradient descent algorithm with a constant step size and random initialization instead. Additionally, our overall algorithm achieves nearly linear sample complexity, $\mathcal{O}(d \mathrm{polylog}(d))$.
</em>
</p>
</details>
						<br>
						Adarsh Barik, Anand Krishna, Vincent Y. F. Tan<br>
						Accepted, IEEE Transactions on Information Theory, 2025
					</li>

					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2208.09449">On Exact Solutions of the Inner Optimization Problem of Adversarial Robustness</a>
</summary>
<p>
<em>
    We propose a robust framework that uses adversarially robust training to safeguard the ML models against perturbed testing data. Our contributions can be seen from both computational and statistical perspectives. Firstly, from a computational/optimization point of view, we derive the ready-to-use exact solution for several widely used loss functions with a variety of norm constraints on adversarial perturbation for various supervised and unsupervised ML problems, including regression, classification, two-layer neural networks, graphical models, and matrix completion. The solutions are either in closed-form, or an easily tractable optimization problem such as 1-D convex optimization, semidefinite programming, difference of convex programming or a sorting-based algorithm. Secondly, from statistical/generalization viewpoint, using some of these results, we derive novel bounds of the adversarial Rademacher complexity for various problems, which entails new generalization bounds. Thirdly, we perform some sanity-check experiments on real world datasets for supervised problems such as regression and classification, as well as for unsupervised problems such as matrix completion and learning graphical models.
</em>
</p>
</details>
						<br>
						Deepak Maurya, Adarsh Barik, Jean Honorio<br>
						Accepted, ICASSP 2025
					</li>

					<li>
<details>
<summary>
                        <a href="https://arxiv.org/abs/2412.10751"> p-Mean Regret for Stochastic Bandits </a>
                        </summary>
<p>
<em>
In this work, we extend the concept of the $p$-mean welfare objective from social choice theory~\cite{moulin2004fair} to study $p$-mean regret in stochastic multi-armed bandit problems. The $p$-mean regret, defined as the difference between the optimal mean among the arms and the $p$-mean of the expected rewards, offers a flexible framework for evaluating bandit algorithms, enabling algorithm designers to balance fairness and efficiency by adjusting the parameter $p$. Our framework encompasses both average cumulative regret and Nash regret as special cases.
We introduce a simple, unified UCB-based algorithm (\textsc{Explore-Then-UCB}) that achieves novel $p$-mean regret bounds. Our algorithm consists of two phases: a carefully calibrated uniform exploration phase to initialize sample means, followed by the UCB1 algorithm of~\citet{auer2002finite}. Under mild assumptions, we prove that our algorithm achieves a $p$-mean regret bound of $\tilde{O}\left(\sqrt{\frac{k}{T^{\frac{1}{2|p|}}}}\right)$ for all $p \leq -1$, where $k$ represents the number of arms and $T$ the time horizon. When $-1 &lt; p &lt; 0$, we achieve a regret bound of $\tilde{O}\left(\sqrt{\frac{k^{1.5}}{T^{\frac{1}{2}}}}\right)$. For the range $0 &lt; p \leq 1$, we achieve a $p$-mean regret scaling as $\tilde{O}\left(\sqrt{\frac{k}{T}}\right)$, which matches the previously established lower bound up to logarithmic factors~\cite{auer1995gambling}. This result stems from the fact that the $p$-mean regret of any algorithm is at least its average cumulative regret for $p \leq 1$.
In the case of Nash regret (the limit as $p$ approaches zero), our unified approach differs from prior work~\cite{barman2023fairness}, which requires a new Nash Confidence Bound algorithm. Notably, we achieve the same regret bound up to constant factors using our more general method.
</em>
</p>
</details>
						<br>
						Anand Krishna, Philips George John, Adarsh Barik, Vincent Y. F. Tan<br>
						Accepted, AAAI 2025
					</li>

                    <li>
<details>
<summary>
                                <a href="https://www.cs.purdue.edu/homes/jhonorio/controversy_cn24.pdf" onclick="return false;"> An SDP Formulation for Minimizing p-th Order Controversy with Unknown Initial Opinions
         </a>
</summary>
<p>
<em>
    In this paper, we investigate the problem of minimizing $p$-th order controversy within a network, assuming a framework of opinion evolution based on the well-established Friedkin-Johnsen (FJ) model. We define $p$-th order controversy as $f_p(L) = s^T (I+L)^{-p} s$, where $s$ represents the vector of users’ fixed albeit undisclosed initial opinions, $I$ is the identity matrix, and $L$ is the graph Laplacian associated with the underlying network. Notably, for the case of $p = 1$, this function transforms into the widely recognized polarization-disagreement index, and for $p = 2$, it aligns with the standard polarization. We focus on minimizing $f_p(L)$ within a novel and realistic framework, where users’ initial opinions $s$ are undisclosed. Due to the undisclosed nature of users’ initial opinions, achieving the exact minimization of $f_p(L)$ proves unattainable within our innovative and practical framework. To address this challenge, we introduce a novel semidefinite programming formulation designed to enable the minimization of the upper bound of $f_p(L)$ without the need for knowledge of initial opinions. Furthermore, our empirical findings demonstrate its effectiveness, surpassing current state-of-the-art methodologies.
</em>
</p>
</details>
                        <br>
						Meher Chaitanya, Adarsh Barik, Jean Honorio<br>
						Accepted, Complex Networks 2024
					</li>

                    <li>
<details>
<summary>
                                <a href="https://arxiv.org/abs/2006.12583"> Exact Support Recovery in Federated Regression with One-shot Communication
         </a>
</summary>
<p>
<em>
Federated learning provides a framework to address the challenges of distributed computing, data ownership and privacy over a large number of distributed clients with low computational and communication capabilities. In this paper, we study the problem of learning the exact support of sparse linear regression in the federated learning setup. We provide a simple communication efficient algorithm which only needs one-shot communication with the centralized server to compute the exact support. Our method does not require the clients to solve any optimization problem and thus, can be run on devices with low computational capabilities. Our method is naturally robust to the problems of client failure, model poisoning and straggling clients. We formally prove that our method requires a number of samples per client that is polynomial with respect to the support size, but independent of the dimension of the problem. We require the number of distributed clients to be logarithmic in the dimension of the problem. If the predictor variables are mutually independent then the overall sample complexity matches the optimal sample complexity of the non-federated centralized setting. Furthermore, our method is easy to implement and has an overall polynomial time complexity.
</em>
</p>
</details>
                        <br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, TMLR 2023
					</li>
					<li>
<details>
<summary>
						<a href="http://arxiv.org/abs/1911.04225"> Provable Computational and Statistical Guarantees for Efficient Learning of Continuous-Action Graphical Games </a>
</summary>
<p>
<em>
In this paper, we study the problem of learning the set of pure strategy Nash equilibria and the exact structure of a continuous-action graphical game with quadratic payoffs by observing a small set of perturbed equilibria. A continuous-action graphical game can possibly have an uncountable set of Nash euqilibria. We propose a $\ell_12$− block regularized method which recovers a graphical game, whose Nash equilibria are the ϵ-Nash equilibria of the game from which the data was generated (true game). Under a slightly stringent condition on the parameters of the true game, our method recovers the exact structure of the graphical game. Our method has a logarithmic sample complexity with respect to the number of players. It also runs in polynomial time.
</em>
</p>
</details>
						<br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, ICASSP 2023
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2206.01167"> Sparse Mixed Linear Regression with Guarantees: Taming an Intractable Problem with Invex Relaxation </a>
</summary>
<p>
<em>
In this paper, we study the problem of sparse mixed linear regression on an unlabeled dataset that is generated from linear measurements from two different regression parameter vectors. Since the data is unlabeled, our task is not only to figure out a good approximation of the regression parameter vectors but also to label the dataset correctly. In its original form, this problem is NP-hard. The most popular algorithms to solve this problem (such as Expectation-Maximization) have a tendency to stuck at local minima. We provide a novel invex relaxation for this intractable problem which leads to a solution with provable theoretical guarantees. This relaxation enables exact recovery of data labels. Furthermore, we recover a close approximation of the regression parameter vectors which match the true parameter vectors in support and sign. Our formulation uses a carefully constructed primal dual witnesses framework for the invex problem. Furthermore, we show that the sample complexity of our method is only logarithmic in terms of the dimension of the regression parameter vectors.
</em>
</p>
</details>
						<br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, ICML 2022
					</li>
					<li>
<details>
<summary>
						<a href="http://arxiv.org/abs/2102.09626"> A Simple Unified Framework for High Dimensional Bandit Problems </a>
</summary>
<p>
<em>
Stochastic high dimensional bandit problems with low dimensional structures are useful in different applications such as online advertising and drug discovery. In this work, we propose a simple unified algorithm for such problems and present a general analysis framework for the regret upper bound of our algorithm. We show that under some mild unified assumptions, our algorithm can be applied to different high dimensional bandit problems. Our framework utilizes the low dimensional structure to guide the parameter estimation in the problem, therefore our algorithm achieves the best regret bounds in the LASSO bandit, as well as novel bounds in the low-rank matrix bandit, the group sparse matrix bandit, and in a new problem: the multi-agent LASSO bandit.
</em>
</p>
</details>
			             <br>
						Li Wenjie, Adarsh Barik, Jean Honorio<br>
						Accepted, ICML 2022
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/1811.06635"> Information Theoretic Limits for Standard and One-Bit Compressed Sensing with Graph-Structured Sparsity </a>
</summary>
<p>
<em>
In this paper, we analyze the information theoretic lower bound on the necessary number of samples needed for recovering a sparse signal under different compressed sensing settings. We focus on the weighted graph model, a model-based framework proposed by Hegde et al. (2015), for standard compressed sensing as well as for one-bit compressed sensing. We study both the noisy and noiseless regimes. Our analysis is general in the sense that it applies to any algorithm used to recover the signal. We carefully construct restricted ensembles for different settings and then apply Fano's inequality to establish the lower bound on the necessary number of samples. Furthermore, we show that our bound is tight for one-bit compressed sensing, while for standard compressed sensing, our bound is tight up to a logarithmic factor of the number of non-zero entries in the signal.
</em>
</p>
</details>
						<br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, ICASSP 2022
					</li>
					<li>
<details>
<summary>
						<a href="http://arxiv.org/abs/2004.01022"> Provable Sample Complexity Guarantees for Learning of Continuous-Action Graphical Games with Nonparametric Utilities </a>
</summary>
<p>
<em>
In this paper, we study the problem of learning the exact structure of continuous-action games with non-parametric utility functions. We propose an $\ell_1$ regularized method which encourages sparsity of the coefficients of the Fourier transform of the recovered utilities. Our method works by accessing very few Nash equilibria and their noisy utilities. Under certain technical conditions, our method also recovers the exact structure of these utility functions, and thus, the exact structure of the game. Furthermore, our method only needs a logarithmic number of samples in terms of the number of players and runs in polynomial time. We follow the primal-dual witness framework to provide provable theoretical guarantees.
</em>
</p>
</details>
                        <br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, ICASSP 2022
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2102.09704"> Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem
 </a>
 </summary>
<p>
<em>
In this paper, we study the problem of fair sparse regression on a biased dataset where bias depends upon a hidden binary attribute. The presence of a hidden attribute adds an extra layer of complexity to the problem by combining sparse regression and clustering with unknown binary labels. The corresponding optimization problem is combinatorial, but we propose a novel relaxation of it as an \emph{invex} optimization problem. To the best of our knowledge, this is the first invex relaxation for a combinatorial problem. We show that the inclusion of the debiasing/fairness constraint in our model has no adverse effect on the performance. Rather, it enables the recovery of the hidden attribute. The support of our recovered regression parameter vector matches exactly with the true parameter vector. Moreover, we simultaneously solve the clustering problem by recovering the exact value of the hidden attribute for each sample. Our method uses carefully constructed primal dual witnesses to provide theoretical guarantees for the combinatorial problem. To that end, we show that the sample complexity of our method is logarithmic in terms of the dimension of the regression parameter vector.
</em>
</p>
</details>
  <br>
						Adarsh Barik, Jean Honorio<br>
						Accepted (Spotlight), NeurIPS 2021
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/2102.10199"> Information-Theoretic Bounds for Integral Estimation </a>
</summary>
<p>
<em>
In this paper, we consider a zero-order stochastic oracle model of estimating definite integrals. In this model, integral estimation methods may query an oracle function for a fixed number of noisy values of the integrand function and use these values to produce an estimate of the integral. We first show that the information-theoretic error lower bound for estimating the integral of a d-dimensional function over a region with l∞ radius r using at most T queries to the oracle function is $\Omega(2^d r^{d+1} \sqrt{d/T})$. Additionally, we find that the Gaussian Quadrature method under the same model achieves a rate of $O(26d r^d/\sqrt{T})$ for functions with zero fourth and higher-order derivatives with respect to individual dimensions, and for Gaussian oracles, this rate is tight. For functions with nonzero fourth derivatives, the Gaussian Quadrature method achieves an upper bound which is not tight with the information-theoretic lower bound. Therefore, it is not minimax optimal, so there is space for the development of better integral estimation methods for such functions.
</em>
</p>
</details>
						 <br>
						Donald Q. Adams, Adarsh Barik, Jean Honorio<br>
						Accepted, ISIT 2021
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/1803.04087"> Learning Discrete Bayesian Networks in Polynomial Time and Sample Complexity </a>
</summary>
<p>
<em>
In this paper, we study the problem of structure learning for Bayesian networks in which nodes take discrete values. The problem is NP-hard in general but we show that under certain conditions we can recover the true structure of a Bayesian network with sufficient number of samples. We develop a mathematical model which does not assume any specific conditional probability distributions for the nodes. We use a primal-dual witness construction to prove that, under some technical conditions on the interaction between node pairs, we can do exact recovery of the parents and children of a node by performing group $\ell_{12}$-regularized multivariate regression. Thus, we recover the true Bayesian network structure. If degree of a node is bounded then the sample complexity of our proposed approach grows logarithmically with respect to the number of nodes in the Bayesian network. Furthermore, our method runs in polynomial time.
</em>
</p>
</details>

						<br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, ISIT 2020
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/1905.12552"> Learning Bayesian Networks with Low Rank Conditional Probability Tables </a>
</summary>
<p>
<em>
In this paper, we provide a method to learn the directed structure of a Bayesian network using data. The data is accessed by making conditional probability queries to a black-box model. We introduce a notion of simplicity of representation of conditional probability tables for the nodes in the Bayesian network, that we call "low rankness". We connect this notion to the Fourier transformation of real valued set functions and propose a method which learns the exact directed structure of a `low rank` Bayesian network using very few queries. We formally prove that our method correctly recovers the true directed structure, runs in polynomial time and only needs polynomial samples with respect to the number of nodes. We also provide further improvements in efficiency if we have access to some observational data.
</em>
</p>
</details>
						 <br>
						Adarsh Barik, Jean Honorio<br>
						Accepted, NeurIPS 2019
					</li>
					<li>
<details>
<summary>
						<a href="https://arxiv.org/abs/1701.07895"> Information Theoretic Limits for Linear Prediction with Graph-Structured Sparsity </a>
</summary>
<p>
<em>
We analyze the necessary number of samples for sparse vector recovery in a noisy linear prediction setup. This model includes problems such as linear regression and classification. We focus on structured graph models. In particular, we prove that sufficient number of samples for the weighted graph model proposed by Hegde and others is also necessary. We use the Fano's inequality on well constructed ensembles as our main tool in establishing information theoretic lower bounds.
</em>
</p>
</details>
						 <br>
						Adarsh Barik, Jean Honorio, Mohit Tawarmalani <br>
						Accepted, ISIT 2017
					</li>
				</ol>
				<p></p>

				<h2 id="teaching" class="subheadertext">Teaching</h2>
				<p class="normaltext">
				</p>
                <ul class="normaltext">
                    <li><a href="courses/cov878/cov878_web.html">[Instructor] COV878 - Special Module in Machine Learning, Semester 1, 2025-26 </a></li>
					<li>[Instructor] MGMT305 - Business Statistics, Summer 2017 <span class="alerttext">(Outstanding Instructor Award)</span> </li>
                    <li>[Teaching Assistant] Statistical Machine Learning, CS578, Fall 2017, Spring 2018, Spring 2020, Spring 2023</li>
                    <li>[Teaching Assistant] Computational Methods in Optimization, CS520, Spring 2021  </li>
                    <li>[Teaching Assistant] Numerical Methods, CS314, Fall 2020, Fall 2021 </li>
                    <li>[Teaching Assistant] Foundation of Computer Science, CS182, Summer 2020, Summer 2021 </li>
					<li>[Teaching Assistant] MGMT670 - Business Analytics, Summer 2016 </li>
					<li>[Teaching Assistant] MGMT306 - Management Science, Spring 2016 </li>
					<li>[Teaching Assistant] MGMT305 - Business Statistics, Fall 2015 </li>
				</ul>


				<h2 id="academic" class="subheadertext">Academic Service</h2>
				<p class="normaltext">
				</p><ul class="normaltext">
					<li> Conferences (Reviewer) - ICLR 2024, NeurIPS 2023, AISTATS 2023, NeurIPS 2022, ICML 2022, AISTATS 2022, AISTATS 2021  </li>
				</ul>

              </section>
            </div>

        <p id="footer"> until peace &copy; 2025 Adarsh</p>
</body></html>
